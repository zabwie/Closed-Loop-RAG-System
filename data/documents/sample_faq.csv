question,answer,category
What is RAG and how does it work?,RAG stands for Retrieval-Augmented Generation. It's a technique that combines large language models with external knowledge retrieval systems. When you ask a question, the system first searches through a database of documents to find relevant information, then passes that information to the language model along with your question. The model uses this retrieved context to generate a more accurate and up-to-date response than it could produce from its training data alone. This approach addresses several key limitations of pure LLM approaches including knowledge cutoff dates, hallucination issues, and lack of domain-specific expertise.,Technical
What are the main components of a RAG system?,A RAG system typically consists of three main components: a document store that contains your knowledge base, a retrieval system that finds relevant documents based on your query, and a generation model (usually an LLM) that synthesizes the retrieved information into a coherent response. The retrieval system often uses vector databases and embedding models to find semantically similar documents. These components work together to provide accurate, context-aware responses that leverage your specific knowledge base rather than relying solely on the model's pre-trained knowledge.,Technical
What is a vector database and why is it important for RAG?,A vector database stores high-dimensional vector representations of text rather than the text itself. These vectors capture the semantic meaning of the content, allowing the system to find documents that are similar in meaning even if they use different words. This is crucial for RAG because it enables more intelligent retrieval than simple keyword matching. Popular vector databases include Pinecone, Weaviate, Chroma, and Milvus. The choice of vector database significantly impacts performance in terms of latency, scalability, and cost.,Technical
How do I upload a document to the RAG system?,To upload a document, send a POST request to the /documents/upload endpoint with the file as multipart form data. You can optionally include metadata like title, author, category, and tags. The system will automatically process the document, split it into chunks, embed the chunks, and index them in the vector database. You'll receive a document_id in the response that you can use to reference the document later. The upload process is asynchronous by default, meaning the response returns immediately while processing happens in the background.,API
How do I query the RAG system?,Send a POST request to the /query endpoint with your natural language question in the request body. You can optionally specify parameters like top_k (number of documents to retrieve), temperature (response randomness), and filters (metadata constraints). The system will retrieve relevant documents and generate an answer. For long-form answers, use the /query/stream endpoint to receive the response in real-time chunks. The query response includes the original query, the AI-generated answer, a list of source documents with their content and similarity scores, and performance metrics.,API
What is the difference between /query and /query/stream endpoints?,The /query endpoint returns the complete response in a single HTTP response, which is simpler to implement but may have higher latency for long answers. The /query/stream endpoint uses Server-Sent Events (SSE) to stream the response in real-time chunks, providing better user experience for complex queries that take longer to generate. The streaming endpoint is particularly useful for chat interfaces and applications where users expect immediate feedback. Both endpoints accept the same request parameters and return similar information, but the streaming approach provides a more responsive experience.,API
How much does it cost to use the RAG system?,Costs depend on several factors: the number of documents stored, the volume of queries, the embedding model used, and the LLM service for generation. Vector database storage costs are typically based on the number of vectors stored. LLM API costs depend on the model and number of tokens processed. For accurate pricing, use the /stats endpoint to monitor your usage and refer to the pricing page for detailed rate information. Optimize costs by using efficient embedding models, implementing caching, optimizing chunk sizes, and monitoring usage patterns.,General
Can I use my own embedding model or LLM?,Yes, the RAG system is designed to be flexible and supports custom embedding models and LLMs. You can configure custom models through the system settings or API. This allows you to use open-source models, fine-tuned models, or models from different providers. Keep in mind that custom models may require additional infrastructure and configuration. Using custom models gives you control over performance, cost, and data privacy. Consider your specific requirements for accuracy, latency, and budget when choosing between default and custom models.,General
How secure is my data in the RAG system?,Data security is a top priority. All documents are encrypted at rest in the vector database. API communications use HTTPS encryption. Access controls and authentication mechanisms protect against unauthorized access. For enterprise deployments, additional security features like VPC isolation, custom encryption keys, and audit logging are available. We comply with major data protection regulations including GDPR and CCPA. Never upload sensitive or confidential information without proper security measures. For highly sensitive data, consider enterprise deployment options with enhanced security features.,General
How do I improve the accuracy of answers?,To improve answer accuracy: ensure your documents are high-quality and comprehensive, use appropriate chunking strategies to preserve context, fine-tune embedding models on your domain data, implement re-ranking to refine retrieval results, use lower temperature settings for more focused responses, provide clear specific queries, and continuously evaluate and iterate based on user feedback. Answer accuracy depends heavily on retrieval quality and document quality. Invest time in curating and maintaining your document collection. Use user feedback to identify areas for improvement and optimize your system accordingly.,General
